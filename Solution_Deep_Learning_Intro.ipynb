{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled25.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "y0VZHs57Dqch"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn \n",
        "from tqdm import tqdm \n",
        "import torchvision \n",
        "\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler"
      ],
      "metadata": {
        "id": "DP1z2yeFEIF4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extraído de https://github.com/rasbt/stat453-deep-learning-ss21/blob/2202699c5fd38af398e2682f289a0868b1b91f0e/L13/code/helper_evaluation.py\n",
        "\n",
        "def compute_history(model, data_loader, device, loss):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        loss_ac, correct_pred, num_examples = 0, 0, 0\n",
        "\n",
        "        for i, (features, targets) in enumerate(data_loader):\n",
        "\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            logits = model(features)\n",
        "\n",
        "            loss_val = loss(logits, targets)\n",
        "            _, predicted_labels = torch.max(logits, 1)\n",
        "\n",
        "            num_examples += targets.size(0)\n",
        "            loss_ac += (loss_val.item())\n",
        "            correct_pred += (predicted_labels == targets.float()).sum()\n",
        "    return correct_pred.float()/num_examples * 100, loss_ac/num_examples * 100"
      ],
      "metadata": {
        "id": "8gQ-xEv3JaOM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição de Hiperparâmetros"
      ],
      "metadata": {
        "id": "9_OeekPzFU6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15 \n",
        "batch_size = 256 \n",
        "val_split = 0.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "I_rrAHI8ELCq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset MNIST"
      ],
      "metadata": {
        "id": "QuIUaFpcFYeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resize_transform = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.Resize((32, 32)),\n",
        "     torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.5,), (0.5,))])\n"
      ],
      "metadata": {
        "id": "oYoDZIBpFYGN"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.MNIST(root='data', train=True, transform=resize_transform, download=True)\n",
        "\n",
        "valid_dataset = torchvision.datasets.MNIST(root='data', train=True, transform=resize_transform)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='data', train=False, transform=resize_transform)"
      ],
      "metadata": {
        "id": "8zr-785PFYED"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Based on \n",
        "\n",
        "num = int(val_split * 60000)\n",
        "train_indices = torch.arange(0, 60000 - num)\n",
        "valid_indices = torch.arange(60000 - num, 60000)\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(valid_indices)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, drop_last=True, sampler=train_sampler)\n",
        "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "7CrMNACOFYAK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dz90AU6oFX-m"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criação da LeNet5"
      ],
      "metadata": {
        "id": "3EZ0OI0mFdhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet5(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes = 10, n_channels=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(n_channels, 6, kernel_size=5),\n",
        "            torch.nn.Tanh(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2),\n",
        "            torch.nn.Conv2d(6, 16, kernel_size=5),\n",
        "            torch.nn.Tanh(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(16*5*5, 120),\n",
        "            torch.nn.Tanh(),\n",
        "            torch.nn.Linear(120, 84),\n",
        "            torch.nn.Tanh(),\n",
        "            torch.nn.Linear(84, num_classes),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        logits = self.classifier(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "YnvmIexmEK_m"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VRXp9uO4HzwO"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8Ob1G0LuHztZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ],
      "metadata": {
        "id": "e4hCHVB3H0N1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LeNet5(num_classes=10)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "12BatR1fH1nj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minibatch_loss_list, train_acc_list, valid_acc_list, train_loss_list, valid_loss_list = [], [], [], [], []\n",
        "\n",
        "criterion = torch.nn.functional.cross_entropy\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    model.train()\n",
        "    for (X_train, y_train) in tqdm(train_loader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        X_train = X_train.to(device)\n",
        "        y_train = y_train.to(device)\n",
        "\n",
        "        output = model(X_train)\n",
        "        loss = criterion(output, y_train)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        minibatch_loss_list.append(loss.item())\n",
        "        \n",
        "    #Validação\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_acc, train_loss = compute_history(model, train_loader, device, criterion)\n",
        "        valid_acc, valid_loss = compute_history(model, valid_loader, device, criterion)\n",
        "        \n",
        "        train_acc_list.append(train_acc.item())\n",
        "        valid_acc_list.append(valid_acc.item())\n",
        "        \n",
        "        train_loss_list.append(train_loss)\n",
        "        valid_loss_list.append(valid_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-24eMyV8H5bt",
        "outputId": "b21b4b09-8977-478e-a48c-5254de705976"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:10<00:00, 18.49it/s]\n",
            "100%|██████████| 187/187 [00:09<00:00, 18.87it/s]\n",
            "100%|██████████| 187/187 [00:09<00:00, 18.73it/s]\n",
            "100%|██████████| 187/187 [00:09<00:00, 18.87it/s]\n",
            "100%|██████████| 187/187 [00:09<00:00, 19.44it/s]\n",
            "100%|██████████| 187/187 [00:09<00:00, 19.20it/s]\n",
            "100%|██████████| 187/187 [00:09<00:00, 19.06it/s]\n",
            "100%|██████████| 187/187 [00:09<00:00, 19.15it/s]\n",
            "100%|██████████| 187/187 [00:09<00:00, 19.36it/s]\n",
            "100%|██████████| 187/187 [00:09<00:00, 19.23it/s]\n",
            "100%|██████████| 187/187 [00:09<00:00, 19.34it/s]\n",
            "100%|██████████| 187/187 [00:10<00:00, 18.59it/s]\n",
            "100%|██████████| 187/187 [00:09<00:00, 19.18it/s]\n",
            "100%|██████████| 187/187 [00:09<00:00, 19.27it/s]\n",
            "100%|██████████| 187/187 [00:09<00:00, 19.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5dpAfua7KaTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ueO4TdeANYnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fFooATiFNYin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kn3SfMjGNYfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bDVxzb9MNYdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BviurYC3NYaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gOn9gXa9NYYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XmDMicCoNYVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "33lcKg-KNYT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HeTsUNA3NqXS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}